-ingress: Layer 7에 해당하는 리소스, HTTP와 HTTPS의 프로토콜에서 동작

ㅁconfigmap volume으로 사용하기
-yaml 파일 읽는 법: mountPath: /etc/config : pod 내부에 있는 디렉토리를 얘기한다(configMap:
name: configmap dev를 mountPath: /etc/config에 마운트 시킨다는 뜻이다)

-/etc/config # ls 하면 변수들이 파일로 존재

-# kubectl exec -it configapp-7d74bc57b8-hk2hn(pod의 이름) -- sh 명령어를 실행 후 프롬프트가 
~ # 상태로 변하면 cd /etc/config로 마운트 시켜
/etc/config # 상태에서 ls -l 명령어를 실행하면 각 변수들이 파일 상태로 존재하는 것을 확인할 수 있다
-http://localhost:30800/volume-config?path=/etc/config/DB_URL등의 변수 내용을 브라우저에서 확인시 실행중인 pod를 보고 worker1인지 2인지
확인 후 해당 인스턴스의 퍼블릭 IP로 접속한다

ㅁSecret
-아이디와 패스워드를 base64로 인코딩을 하여 보안에 중점을 둔 서비스
-echo cGFzc3dvcmQ= | base64 --decode를 했을 때 값과 프롬프트가 같이 나오는 것은 띄어쓰기를 하라는 명령어를 yaml내에 넣지않아 같이 나온다
(passwordroot@~ 같이)

ㅁTLS 인증서 Secret으로 사용하기
-2048 bit RSA private key의 2048비트로 이루어진 프라이빗 키인 tls.key가 생성되고, CERTIFICATE(인증서)인 tls.crt가 생성된다.
cat tls.key, cat tls.crt로 확인할 수 있다

■Chapter 10 Kubernetes Volume(컨테이너에서 중요한 개념)
-hostpath: worker1과 worker2 같은 node에 매번 같이 생성된다(pod가 어디에 생성될지 모르므로..??)
-볼륨의 종류 중 임시볼륨(pod를 삭제하면 같이 삭제되는 볼륨), 로컬 볼륨보다 네트워크 볼륨이 중요하다
-hostpath 볼륨을 사용했을 때 worker1과 2의 node에서 mydata라는 디렉토리를 같이 생성했지만, 실제 pod는 worker1에서 생성되었고
worker1에 마운트 되었기 때문에 Master에서 deploy에서 nginx-deplyment를 삭제했더라도 woker1에서는 남아있어 txt파일이 읽혀졌다
결론은 worker1과 worker2 모두 디렉토리는 생성은 했지만 마운트 된 디렉토리는 worker1이기 때문에 마스터에서 삭제를 해도 worker2에는 마운트가 되어있지
않아 마스터에서 만든 txt 파일을 볼 수 없었지만 worker1에서는 남아있어 보였다(hostpath의 마운트 개념을 모른 상태로 worker1,2동시에 hostpath가 생성되는
줄 알고 햇깔렸다)
-네트워크 볼륨
iSCSI
NFS(같은 OS 간의 파일 이동, Samba는 다른 OS라도 파일 이동을 시켜주는 프로그램)
cephFS(redhat에서 자주 사용)
glusterFS(redhat에서 자주 사용)
-PV와 PVC(도커에는 없고 쿠버네티스와 오픈시프트에는 있는 개념)
-PV와 Pysical disk(물리적 디스크)는 일반사용자가 생성하지 못한다
-PV: 물리적 개념, PVC: 논리적 개념
-kubectl exec -it shared-volumes --container redis -- /bin/bash
-PVC가 PV를 찾는 방법은 accessModes와 resources의 조건을 통해 찾는다, 이름으로 찾지 않고 조건이 똑같은 PV가 있으면 아무 PV나 사용을 하고 
사용하고 있는 PV상태는 Available상태가 되고 다른 PV는 Bound상태가 된다(중요한 포인트는 PVC는 PV를 이름으로 찾지 않고 엑세스 모드와 리소스의 조건으로
찾는다)
-교제에 나온 NFS서버 구축하기는 CentOS 8 기준으로 나와있기 때문에, 구글에서 How to install Ubuntu 20 NFS server 등으로 찾아 설치해보자
혹은 GPT에 질문하자
-df -hT명령어로 사용중인 HDD의 용량을 확인 할 수 있다
-apt-get install nfs-kernel-server => mkdir /volume-nfs => chown nobody:nogroup /volume-nfs/ =>chmod 755 /volume-nfs/
=> vim /etc/exports에서 아랫줄에 /volume-nfs *(rw,sync,no_subtree_check) *대신 172.31.0.0/16으로 설정 => 메모장ㅇ에 있는대로 하다가
mount 옆 IP는 마스터의 프라이빗 아이피로 /volume-nfs 디렉토리를 마운트 시킨다
-네트워크의 순서: Pod -> PVC -> PV -> NFS(실습했을 때 설치한 프로그램)
-nfs-common은 연결하려는 노드들(worker1,2)에 설치를 하고 마운트:아이피는 마스터의 프라이빗 아이피를 넣고 마운트를 한다
-pv를 생성할 때 
nfs:
path: /volume nfs
server: 213.0.113 .3 (이 부분은 master의 프라이빗 아이피를 넣어준다)
-pod를 생성하기 전 worker1이나 2에서 마운트가 됐는지 테스트를 먼저 한 후에 apply해보자
( mount 172.31.28.124:/volume-nfs /mnt/master 명령어로) cat /etc/hosts로 master의 ip를 입력
-컨테이너가 만들어지지 않으면 pod 삭제 후 pvc먼저 삭제한 후에 pv를 지워야 한다
-kubectl get events 명령어로 생성같은 이벤트를 볼 수있고, mount가 왜 안되는지까지 볼 수 있으니 참고해야 한다
-pvc, pv, pod를 삭제한 후에 다시 만들었을 때 (pod만 nginx2로 다르게)pod 내부로 접속해서 index.html이 남아 있는지 확인 - 디렉토리의 재활용
-변경시 메타데이터 쪽에 name만 변경하면 된다고 한다

■Pod Scheduling, 인증& 권한
-Pod Scheduling Affinity Anti affinity의 개념은 어렵지만, 중요하다고 함
-affinity와 anti-affinity는 반대의 개념
-Affinity: Pod 지정한 레이블 가진 Pod 들 있는 노드로 Scheduling / 같은조건의 pod들이 있는 곳에 구성 
-Anti affinity: 같은 조건 가진 Pod 피해서 다른 노드에 Pod 배포 / 다른조건의 pod에 구성
ㅁTaint & Toleration
-taint가 있는 node들은 기본적으로 pod 생성이 되지 않는다(제한한다)
-kubernetes 는 각 노드에 taint 설정 감염으로부터 용인(Toleration)된 POD 만 해당 node 에서 실행
ㅁCordon & Drain
-Cordon: 특정 노드 선택하여 스케줄 대상에서 제외
-Drain: cordon 과 동일하게 동작 SchedulingDisabled 된 노드에 남아있는 Pod 모두 삭제하고 재생성

ㅁaffinity 실습
-required: 강제조건 / preferred: 선택조건
-operator: Gt(Greater than: ~보다 큰 / Lt: Less than: ~보다 작은)
ㅁ안티어피니티
-yaml파일에서 replicas를 3으로 설정한 후 yaml파일을 apply 하면 
빈 pod 중 worker1에 pod를 생성을 하고, 같은 조건의 pod를 피해야 하니 worker2에 pod를 생성을 했는데 나머지 하나의 replica는 두 개의 worker들이
키:값이 동일하니 pending(기다리는 중)상태로 뜰 수 밖에 없다 - anti라고 해서 yaml실행 하자마자 3개의 replicas들이 pending이 뜨는게 아니고
빈 pod를 채우다가 동일 조건의 키:값을 피해 다른 node들에 생성을 시작하는 것
-pod affinity. yaml의 podAntiAffinity의 강제조건(required)과 podAffinity의 강제조건을 둘 다 만족해야 하고 생성한 deploymentㅇ의 값도 
app: web-store니 같은 node안에 두 개의 pod가 동시에 만들어질 수 없기에 각기 다른 node(worker)에 생성 되야 한다.
yaml파일의 읽는 개념이 어렵다 / tip. aws와 쿠버네티스의 개념을 잘 알아야 한다고 한다